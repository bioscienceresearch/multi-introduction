{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import treeswift\n",
    "from utils import *\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta as dt, timedelta\n",
    "import datetime\n",
    "from collections import Counter\n",
    "import time\n",
    "import numpy.random as rng\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Primary analysis (3.47-day doubling time, 15% ascertainment rate) tree sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Less than 787 taxa: 0.014545\n",
      "More than 1000 taxa: 0.983636\n",
      "More than 5000 taxa: 0.967273\n"
     ]
    }
   ],
   "source": [
    "clade_analyses_CC_dir = '0.28TF/clade_analyses_CC/'\n",
    "clade_analyses_AB_dir = '0.28TF/clade_analyses_AB/'\n",
    "\n",
    "clade_analyses_CC_d = dict()\n",
    "for path in sorted(os.listdir(clade_analyses_CC_dir)): # pool all C/C clade analyses together \n",
    "    clade_analysis_path = clade_analyses_CC_dir + path\n",
    "    key = path.split('_')[0]\n",
    "    num_taxa = 0\n",
    "    for line in open(clade_analysis_path):\n",
    "        l = line.strip().strip(']').split('[')\n",
    "        clade_size = int(l[0].strip()) # make each clade size (including single leaves) an integer\n",
    "        num_taxa += clade_size\n",
    "    clade_analyses_CC_d[key] = num_taxa\n",
    "clade_analyses_CC_d = {int(k):clade_analyses_CC_d[k] for k in clade_analyses_CC_d}    \n",
    "clade_analyses_CC_sizes = list(clade_analyses_CC_d.values())\n",
    "\n",
    "print('Less than 787 taxa: %f' % (sum([x < 787 for x in clade_analyses_CC_sizes])/1100))\n",
    "print('More than 1000 taxa: %f' % (sum([x > 1000 for x in clade_analyses_CC_sizes])/1100))\n",
    "print('More than 5000 taxa: %f' % (sum([x > 5000 for x in clade_analyses_CC_sizes])/1100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree shapes and bayes factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unconstrained_results = np.array([1.68,80.85, 10.32, 0.92])/100 # linB, linA, C/C, T/T\n",
    "recCA_results = np.array([77.28, 8.18, 10.49, 3.71])/100 # linB, linA, C/C, T/T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bf(asr_results, simulation_results):\n",
    "    # Let t_p be a polytomy, t_1C be one clade, and t_2c be two clades\n",
    "    # trees are in the order\n",
    "    # (t_p, t_1C, t_2C, (t_p,(t_p,t_1C,t_2C)), (t_1C,(t_p,t_1C,t_2C)), (t_2c,(t_p,t_1C,t_2C)))\n",
    "    compatibility_matrix = np.array([np.array([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0]), \\\n",
    "                                     np.array([0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0]),  \\\n",
    "                                     np.array([0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0]),  \\\n",
    "                                     np.array([0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0])]  \\\n",
    "                                    )\n",
    "    # A matrix of conditional probabilities\n",
    "    # Each row stores the vector Pr(S_MRCA | \\btau)\n",
    "    pr_s_mrca_given_tree = compatibility_matrix.T\n",
    "    pr_s_mrca_given_tree = np.array([x/sum(x) if sum(x) > 0 else x for x in pr_s_mrca_given_tree]) # if tree not associated with any haplotype, just keep the row as all 0s\n",
    "    \n",
    "    # Order: S_A, S_B, S_{C/C}, S_{T/T}\n",
    "    pr_s_mrca_given_data = np.array(asr_results)/sum(asr_results)\n",
    "    unnormalized_pr_data_given_s_mrca = pr_s_mrca_given_data.copy()\n",
    "    \n",
    "    # FAVITES simulation information\n",
    "    # the 3 trees are in the order (t_p, t_1C, t_2C)\n",
    "    pr_3_topos = np.array(simulation_results)/sum(simulation_results)\n",
    "    pr_trees_given_I1 = np.concatenate([pr_3_topos, np.array([0]*9)])\n",
    "    pr_trees_given_I2 = np.concatenate([np.array([0]*3), np.outer(pr_3_topos, pr_3_topos).flatten()])\n",
    "    \n",
    "    # Equal prior probability of 1 or 2 intros\n",
    "    pr_I1 = 0.5\n",
    "    pr_I2 = 0.5\n",
    "    \n",
    "    pr_s_mrca_and_I1 = np.dot([np.dot(pr_s_mrca_given_tree.T[i], pr_trees_given_I1) for i in range(0,4)], pr_I1) # dot product of P(haplotype|tree) (column) and P(trees|I_n), scaled by P(I_n)\n",
    "    pr_s_mrca_and_I2 = np.dot([np.dot(pr_s_mrca_given_tree.T[i], pr_trees_given_I2) for i in range(0,4)], pr_I2)\n",
    "    \n",
    "    posterior_odds = np.dot(unnormalized_pr_data_given_s_mrca, pr_s_mrca_and_I2) / np.dot(unnormalized_pr_data_given_s_mrca, pr_s_mrca_and_I1)\n",
    "    prior_odds = pr_I2/pr_I1\n",
    "    BF = posterior_odds/prior_odds\n",
    "\n",
    "    return(BF)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clade_analysis_updated(clade_analyses_CC_dir, clade_analyses_AB_dir, label, min_polytomy_size=100, _print_=False):\n",
    "    \n",
    "    # C/C analyses focus on 1-mutation clades\n",
    "    # therefore, all descendant lineages are included:\n",
    "    #   -1 = unmutated leaf\n",
    "    #    1 = mutated leaf separate from a clade\n",
    "    #   >1 = clade\n",
    "    clade_analyses_CC_d = dict()\n",
    "    for path in sorted(os.listdir(clade_analyses_CC_dir)): # pool all C/C clade analyses together \n",
    "        clade_analysis_path = clade_analyses_CC_dir + path\n",
    "        key = path.split('_')[0]\n",
    "        clade_analyses_CC_d[key] = {'clade_sizes': [], 'subclade_sizes': []}\n",
    "        for line in open(clade_analysis_path):\n",
    "            l = line.strip().strip(']').split('[')\n",
    "            clade_size = int(l[0].strip()) # make each clade size (including single leaves) an integer\n",
    "            subclade_sizes = [int(x) for x in l[1].strip().replace(' ', '').split(',')] # put each subclade size (including single leaves) into a list        \n",
    "            clade_analyses_CC_d[key]['clade_sizes'].append(clade_size)\n",
    "            clade_analyses_CC_d[key]['subclade_sizes'].append(subclade_sizes)\n",
    "    clade_analyses_CC_d = {int(k):clade_analyses_CC_d[k] for k in clade_analyses_CC_d}    \n",
    "\n",
    "\n",
    "    # A/B analyses focus on 2-mutation clades\n",
    "    # only 2-mutation clades are included \n",
    "    clade_analyses_AB_d = dict()\n",
    "    for path in sorted(os.listdir(clade_analyses_AB_dir)): # pool all A/B clade analyses together \n",
    "        clade_analysis_path = clade_analyses_AB_dir + path\n",
    "        key = path.split('_')[0]\n",
    "        clade_analyses_AB_d[key] = {'clade_sizes': [], 'subclade_sizes': []}\n",
    "        for line in open(clade_analysis_path):\n",
    "            l = line.strip().strip(']').split('[')\n",
    "            clade_size = int(l[0].strip()) # make each clade size (including single leaves) an integer\n",
    "            subclade_sizes = [int(x) for x in l[1].strip().replace(' ', '').split(',')] # put each subclade size (including single leaves) into a list        \n",
    "            clade_analyses_AB_d[key]['clade_sizes'].append(clade_size)\n",
    "            clade_analyses_AB_d[key]['subclade_sizes'].append(subclade_sizes)\n",
    "    clade_analyses_AB_d = {int(k):clade_analyses_AB_d[k] for k in clade_analyses_AB_d}    \n",
    "    \n",
    "    \n",
    "    # C/C analysis\n",
    "    cc_count = 0 # how often are there only two 1-mutation clades and no other descendants from the root?\n",
    "    cc_count_1perc = 0 # how often are there only two 1-mutation clades, each constituting more than 1% of taxa, and no other descendants from the root?\n",
    "    cc_count_30perc = 0 # how often are there only two 1-mutation clades, each constituting more than 30% of taxa, and no other descendants from the root?\n",
    "    cc_count_30perc_twoPolytomies = 0 # how often are there only two 1-mutation clades, each constituting more than 30% of taxa AND with a polytomy at the base of each clade, and no other descendants from the root?\n",
    "    for run in clade_analyses_CC_d:\n",
    "        clade_sizes = clade_analyses_CC_d[run]['clade_sizes'].copy()\n",
    "        subclade_sizes = [len(x)>=min_polytomy_size for x in clade_analyses_CC_d[run]['subclade_sizes'].copy()]\n",
    "        if len(clade_sizes) == 2: # if there are more than two descendants, including individual leaves, skip\n",
    "            if sum([clade_size > 1 for clade_size in clade_sizes]) == 2: # make sure each clade is an actual clade\n",
    "                cc_count += 1\n",
    "                if min(clade_sizes) > (sum(clade_sizes)*0.01): # make sure each clade is >1%\n",
    "                    cc_count_1perc += 1\n",
    "                if min(clade_sizes) > (sum(clade_sizes)*0.30): # make sure each clade is >30%\n",
    "                    cc_count_30perc += 1\n",
    "                    # does the clade have a polytomy at the base? \n",
    "                    if False not in subclade_sizes:\n",
    "                        cc_count_30perc_twoPolytomies += 1\n",
    "    \n",
    "    # A/B analysis\n",
    "    ab_count_30perc = 0 # interested in 2 mutations clade that are at least 30% of all taxa\n",
    "    ab_count_30perc_polytomy = 0 # interested in 2 mutations clade that are at least 30% of all taxa + has a basal polytomy\n",
    "    ab_count_30perc_twoPolytomies = 0 # interested in 2 mutations clade that are at least 30% of all taxa + has a basal polytomy + polytomy at 2 mutation clade\n",
    "    lower_constraint = 0.3 # the 2-mutation clade must be at least 30% of all taxa\n",
    "    upper_constraint = 0.7 # the 2-mutation clade must be at most 70% of all taxa\n",
    "    \n",
    "    for run in clade_analyses_AB_d:\n",
    "        num_leaves = sum(clade_analyses_CC_d[run]['clade_sizes'])\n",
    "        base_polytomy_size = len(clade_analyses_CC_d[run]['clade_sizes'])\n",
    "        clade_sizes = clade_analyses_AB_d[run]['clade_sizes']\n",
    "        subclade_sizes = clade_analyses_AB_d[run]['subclade_sizes'].copy()\n",
    "        if not clade_sizes: # no 2 mutation clades\n",
    "            continue\n",
    "        if max(clade_sizes) >= lower_constraint*num_leaves and max(clade_sizes) <= upper_constraint*num_leaves: # clades match size restrictions\n",
    "            if len(clade_sizes) == 1:\n",
    "                ab_count_30perc += 1\n",
    "                if base_polytomy_size >= min_polytomy_size: # basal polytomy\n",
    "                    ab_count_30perc_polytomy += 1\n",
    "                    if len(subclade_sizes[0]) >= min_polytomy_size: # two-mutation clade has polytomy\n",
    "                        ab_count_30perc_twoPolytomies += 1\n",
    "            else:\n",
    "                clade2 = sorted(clade_sizes, reverse=True)[1]\n",
    "                ab_count_30perc += 1\n",
    "                if base_polytomy_size >= min_polytomy_size: # basal polytomy\n",
    "                    ab_count_30perc_polytomy += 1\n",
    "                    max2mutCladeLoc = clade_sizes.index(max(clade_sizes))\n",
    "                    if len(subclade_sizes[max2mutCladeLoc]) >= min_polytomy_size: # two mutation clade has polytomy\n",
    "                        ab_count_30perc_twoPolytomies += 1\n",
    "\n",
    "    # polytomy: checking if there is a polytomy at the base of the tree; can do this using the C/C analysis\n",
    "    min_polytomy_descendants = min_polytomy_size\n",
    "    count_atLeastMinDescendants = 0\n",
    "    for run in clade_analyses_CC_d:\n",
    "        clade_sizes = clade_analyses_CC_d[run]['clade_sizes']\n",
    "        if len(clade_sizes) >= min_polytomy_descendants: \n",
    "            count_atLeastMinDescendants += 1\n",
    "    polytomy_result = count_atLeastMinDescendants/1100\n",
    "    \n",
    "    \n",
    "    # calculate bayes factors\n",
    "    cc_result = cc_count_30perc_twoPolytomies/1100\n",
    "    ab_result = ab_count_30perc_twoPolytomies/1100\n",
    "    constraint_text = 'Primary'\n",
    "\n",
    "    simulation_results = [polytomy_result, ab_result, cc_result]\n",
    "    unconstrained_results = np.array([1.68, 80.85, 10.32, 0.92])/100 # linA, linB, C/C, T/T\n",
    "    recCA_results = np.array([77.28, 8.18, 10.49, 3.71])/100 # linA, linB, C/C, T/T\n",
    "    bf_unconstrained = calculate_bf(unconstrained_results, simulation_results)\n",
    "    bf_recCA = calculate_bf(recCA_results, simulation_results)\n",
    "    \n",
    "    if _print_ == True:\n",
    "        print('Proportion with a polytomy at the base: %f\\n' % (count_atLeastMinDescendants/1100))\n",
    "        print('C/C clade analysis: \\n\\tGeneral result: %f\\n\\t1%% min clade size: %f\\n\\t30%% min clade size: %f\\n\\tAnd polytomy at base of each clade: %f\\n' % (cc_count/1100, cc_count_1perc/1100, cc_count_30perc/1100, cc_count_30perc_twoPolytomies/1100))\n",
    "        print('A/B clade analysis:\\n\\tGeneral result: %f\\n\\tWith basal polytomy: %f\\n\\tAnd with polytomy at 2-mut clade: %f\\n' % (ab_count_30perc/1100, ab_count_30perc_polytomy/1100, ab_count_30perc_twoPolytomies/1100))\n",
    "        print('Unconstrained Bayes factor: %f' % bf_unconstrained)\n",
    "        print('recCA Bayes factor: %f' % bf_recCA)\n",
    "\n",
    "    return [label, min_polytomy_size, \"{:.1f}\".format(cc_result*100), \"{:.1f}\".format(ab_result*100), \"{:.1f}\".format(polytomy_result*100), \"{:.1f}\".format(bf_unconstrained), \"{:.1f}\".format(bf_recCA)]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion with a polytomy at the base: 0.475455\n",
      "\n",
      "C/C clade analysis: \n",
      "\tGeneral result: 0.105455\n",
      "\t1% min clade size: 0.067273\n",
      "\t30% min clade size: 0.014545\n",
      "\tAnd polytomy at base of each clade: 0.000000\n",
      "\n",
      "A/B clade analysis:\n",
      "\tGeneral result: 0.108182\n",
      "\tWith basal polytomy: 0.040909\n",
      "\tAnd with polytomy at 2-mut clade: 0.030000\n",
      "\n",
      "Unconstrained Bayes factor: 9.571564\n",
      "recCA Bayes factor: 9.824011\n"
     ]
    }
   ],
   "source": [
    "# 3.5DT\n",
    "clade_analyses_CC_dir = '0.28TF/clade_analyses_CC/'\n",
    "clade_analyses_AB_dir = '0.28TF/clade_analyses_AB/'\n",
    "results = clade_analysis_updated(clade_analyses_CC_dir, clade_analyses_AB_dir, '3.5 DT', min_polytomy_size=100, _print_=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/0.38TF/clade_analyses_CC/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d406e87e63e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclade_analyses_CC_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/0.38TF/clade_analyses_CC/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mclade_analyses_AB_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/0.38TF/clade_analyses_AB/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mall_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclade_analysis_updated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclade_analyses_CC_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclade_analyses_AB_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2.5 DT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_polytomy_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_print_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# 3.5DT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-6a90350af4cc>\u001b[0m in \u001b[0;36mclade_analysis_updated\u001b[0;34m(clade_analyses_CC_dir, clade_analyses_AB_dir, label, min_polytomy_size, _print_)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#   >1 = clade\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mclade_analyses_CC_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclade_analyses_CC_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# pool all C/C clade analyses together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mclade_analysis_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclade_analyses_CC_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/0.38TF/clade_analyses_CC/'"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "\n",
    "# 2.5DT\n",
    "clade_analyses_CC_dir = '/0.38TF/clade_analyses_CC/'\n",
    "clade_analyses_AB_dir = '/0.38TF/clade_analyses_AB/'\n",
    "all_results.append(clade_analysis_updated(clade_analyses_CC_dir, clade_analyses_AB_dir, '2.5 DT', min_polytomy_size=100, _print_=False))\n",
    "\n",
    "# 3.5DT\n",
    "clade_analyses_CC_dir = '/0.28TF/clade_analyses_CC/'\n",
    "clade_analyses_AB_dir = '/0.28TF/clade_analyses_AB/'\n",
    "all_results.append(clade_analysis_updated(clade_analyses_CC_dir, clade_analyses_AB_dir, '3.5 DT', min_polytomy_size=100, _print_=False))\n",
    "\n",
    "# # 4.5DT\n",
    "clade_analyses_CC_dir = '/0.22TF/clade_analyses_CC/'\n",
    "clade_analyses_AB_dir = '/0.22TF/clade_analyses_AB/'\n",
    "all_results.append(clade_analysis_updated(clade_analyses_CC_dir, clade_analyses_AB_dir, '4.5 DT', min_polytomy_size=100, _print_=False)) \n",
    "\n",
    "# # 3.5 DT, low asc\n",
    "clade_analyses_CC_dir = '/0.295TF_0.05r/clade_analyses_CC/'\n",
    "clade_analyses_AB_dir = '/0.295TF_0.05r/clade_analyses_AB/'\n",
    "all_results.append(clade_analysis_updated(clade_analyses_CC_dir, clade_analyses_AB_dir, '3.5 DT, low asc', min_polytomy_size=100, _print_=False))\n",
    "\n",
    "# # 3.5 DT, high asc\n",
    "clade_analyses_CC_dir = '/0.255TF_0.25r/clade_analyses_CC/'\n",
    "clade_analyses_AB_dir = '/0.255TF_0.25r/clade_analyses_AB/'\n",
    "all_results.append(clade_analysis_updated(clade_analyses_CC_dir, clade_analyses_AB_dir, '3.5 DT, high asc', min_polytomy_size=100,  _print_=False))\n",
    "\n",
    "# 3.5DT sensitivity analyses\n",
    "clade_analyses_CC_dir = '/0.28TF/clade_analyses_CC/'\n",
    "clade_analyses_AB_dir = '/0.28TF/clade_analyses_AB/'\n",
    "all_results.append(clade_analysis_updated(clade_analyses_CC_dir, clade_analyses_AB_dir, '3.5 DT', min_polytomy_size=20, _print_=False))\n",
    "all_results.append(clade_analysis_updated(clade_analyses_CC_dir, clade_analyses_AB_dir, '3.5 DT', min_polytomy_size=50, _print_=False))\n",
    "all_results.append(clade_analysis_updated(clade_analyses_CC_dir, clade_analyses_AB_dir, '3.5 DT', min_polytomy_size=200, _print_=False))\n",
    "all_results.append(clade_analysis_updated(clade_analyses_CC_dir, clade_analyses_AB_dir, '3.5 DT', min_polytomy_size=500, _print_=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_df = pd.DataFrame(all_results, columns = ['Analysis', 'Min. polytomy size', 'C/C', 'A/B', 'Polytomy', 'Unconstrained', 'recCA'])\n",
    "pooled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pooled_df.to_csv('bayes_factors.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
